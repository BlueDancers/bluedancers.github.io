---
title: chatGPT一些学习记录
categories:
  - 日常
tags:
  - chatGPT
toc: true
date: 2023-03-21
---



## 最早

​	自然语言处理模型，最早出圈的是2018年google发布的bert，他是一个双向Transformer编码的过程（完形填空），可以对一段文本做到深入的理解与分析，并生成高质量的语言表达。

​	那他究竟是什么原理呢？

​	从一段文本中，随机扣掉一部分字，然后模型不断学习此处到底应该填写什么，**所谓学习与训练就是从大量的数据中学习理解复杂的上下文关系。**

​	注：学习 无监督学习 训练 有监督学习

​	bert的出现推动了自然语言处理领域的发展，在bert出现之前，我们都是基于规则与浅层神经网络（关键字理解）去解决，bert的出现使得深度学习在自然语言处理上得到了更加广泛的使用。

​	在bert出现之前，openid就出版了GPT1，GPT1也是基于Transformers进行实现，但是gpt没有走双向Transformers的方向，而是走类似单字接龙的方向，由长文单字接龙的自回归所生成，不过GPT1并没有出现bert的惊人效果，所以早期的GPT没啥热度。



## GPT2 GPT3

​	基于bert的成就，大家都看到深度学习在语言模型上的可行性，于是大家都开始各种预训练模型，完形填空，判断题，改错题，GPT2、3也确实是这个思路，开始提升训练数据量，这个成果我们称之为开卷有益（无监督学习）。

​	让chatGPT对海量互联网文本做单字接龙，以扩充模型的词汇量、语言只是、世界的信息与知识。是chatGPT从哑巴鹦鹉变成了脑容量超大的懂王鹦鹉，这个和人类的学习其实很相似，小孩刚刚来到这个解释，就是不断的输入信息。

​	经过巨量的数据训练之后，我们下一步就需要情景学习（**in-context learning**），这两个阶段是NLP模型的基础工作流程，简单来说，就是让chatGPT对优质对话范例做单字接龙，以规范回答的对话模式和对话内容。是chatGPT变成懂规矩的博学鹦鹉，这里的场景标记几个就够了，机器一旦学会了规范，便可以应对此类所有问题。



## 关于强化学习（RL）

​	几年前，alpha go击败了柯洁，几乎说明了，经过强化学习，机器可以做到近乎人类极限的程度，但是这样的场景有一个前提，一个非常明确的奖惩机制，alpha go并不理解围棋规则，但是他可以接收到成功、失败的反馈，模型会根据反馈不断调整策略，已达到更多次成功的目的。

​	我们可以看到alpha go的强化学习规则书相对简单的，因为目标明确，而在NLP领域强化学习却很难实现。

​	因为NLP是语言，语言是没有固定的对错，这就意味着无法设定奖惩机制，除非人工来做反馈。

​	openid还就这么干了，这被称之为 **RLHF（Reinforcement Learning from Human Feedback）**

​	openid公司找了40家外包公司，不断帮助gpt筛选什么是好的，什么是坏的，通过这种方式构建了一个奖励（reward）模型。

​	这样训练是为了让他学会举一反三的规律，开始出现类似人类的价值价值观与思考模式。

​	于是gpt通过这样奖励模型，开始感知到真实世界，开始了与真实世界的拟合。甚至可以说奖励模型就是gpt的母体，他对人类的理解不取决于世界，而是取决于模型。

​	这样引导学习的方式，在超大模型上出现惊人的效果，gpt在巨量的数据与规范 + 手动标注引导下，变成了看起来有心智的超级人工智能，甚至出现了理解、例子、思维链的能力。

​	这样的方式也是存在缺点的，比如缺点是可能混淆记忆，无法直接查看个更新所学，并且高度依赖学习材料

<img src="https://www.vkcyan.top/FgOCZQEw2fTJZsn40Q_b5KP5EvZy.png" style="zoom:50%;" />



## GPT对现今社会的可能影响

​	chatGPT是里程碑的产品，就像流浪地球里面的行星发动机，刚开始大家都都不知道其潜力，经过验证后，地球上迅速出现了几万座行星发动机，并完成了最后的流浪地球，chatGPT这样大语言模型就像最开始几座行星发动机 ，最终也将会形成改变我们生活的推力

​	合理的使用大预言模型，就可以让一个普通人快速准确的触及各行各业的平均知识， 以及具体解法

​	所以我们可以畅想，未来我们也许不在需要学习如何打架，只需要对gpt发号施令，让机器去具体的做事即可

​	除了chatGPT本身具备对人类的能力输出，chatGPT具备我们人类不具备的一点，那就是他可以在语言中学会世界知识。

​	人脑是有限的，寿命是有限，个体死亡，一切都会消失，直到语言的发明，打通了过去与未来，知识以文字作为载体传承至今，一直以来我们通过纸质、或者其他物理形式进行存储，其效率是有限的，直到计算机的出现，我们开始用电子文档替代纸质， 信息传递的效率飞速提升，但是同时也带来了信息处理的巨大成本。

​	所以人工智能领域的自然语言技术应运而生（NLP），其目标就是让机器理解人类的语言，协助人类处理工作，这是人工智能领域非常重要的发展方向，虽然这些年发展缓慢，但是依旧被很多公司加以厚望，因为机器不需要作息，没有情绪，效率极高，可以为企业节省大量的成本。



## 与行业结合的可能性

- 搜索引擎，帮助用户更加精准的筛选信息
- 笔记工作结合，辅助阅读与写作
- 办公软件结合，辅助文字处理，数据分析，演示制作
- 教育培训结合，指定学习计划，寻找学习资料
- 开发工具结合，辅助编写业务代码，调试纠错
- 客服系统结合，全天候问答，
- 视频会议结合，会议记录，总结，谈话查找
- 审核机制结合，少选评论，统计舆论，给出提醒
- 行业顾问，提供法律，医疗，健身等等建议
- 社交媒体结合，帮助寻找兴趣相投的用户与话题
- 与视频音乐结合，个性化推荐视频、音乐、小说、动漫
- 游戏剧情结合，让NPC给玩家带来更加灵活的对话体验



## GPT具备对文字行业巨大的改造潜力

- 学术界 创造知识
- 教育行业 传承知识
- 新闻行业 传播知识

​	还有对传统教育行业的巨大挑战，不是GPT可以给学生写作业，而是对现有的人才教育模式存在很大的冲击。

​	按照现在的教育模式（应试教育，传授既有知识）出来的学生，还能应对未来的5-10年的社会需求吗？

​	其实现在已经不太符合了，因为社会变化太快了，以前人们可以通过学习的知识终生受益，现在能满足毕业后5年其实都比较难了，因为知识过时的太快了，人们就需要不断学习新的知识。

​	正因如此，随着互联网时代的到来，终生学习的理念开始被人们推崇，同时教育模式也开始以【培养学习能力与创造能力】为主了，只有这样才能适应不断变化的时代。

 	试想一下你学习了20年，掌握了一些知识，但是GPT却可以瞬间替代你完成你的能力，怎么办？以后人人都有一个熟读人类既有知识的超级大脑，市场不会因为学校的禁用，而集体不使用。

​	任何事物都存在两面性，好的一面是，GPT将方便人来对既有知识进行集成，推进教育去培养高层次人才



## GPT对网络安全的巨大挑战

​	因为GPT的第一阶段，需要大量录入信息，这其中的信息难免会存在一个坏的信息，亦或者军事机密等等信息，所以国家与国家之间很难愿意共享数据，这也意味着，在不就得将来每个有实力的国家都会自己研发大语言模型

​	同时在应用层面，大预言模型将会像口语、文字、电脑、互联网对社会进行再一次改造。



## 社会影响

​	只有会工具的人取代不会使用工具的人，没有工具取代人的说法，所以真正需要害怕的是，我们成为无法成为使用工具的人，时代的车轮势不可挡，抵触新工具就意味着落后，学20年干一辈子的时代已经逐渐远去，我们身处加速时期，必须学会终生学习

​	GPT正在改变人类群体应用知识的方式与继承知识的方式，甚至未来可能会形成人机合作的科研，改变人类创造知识的方式，甚至步入下一个文明形态。

​	每个人的学习能力与理解能力，将是驾驭这项技术的瓶颈，也意味着是否可以充分发挥这一技术的优势，



## 名词解释

**bert（Bidirectional Encoder Representations from Transformers 双向编码变换器）**

​	BERT 基于 Transformers 模型架构，使用双向编码器来训练深度双向表示，并在多项自然语言处理任务上实现了最先进的性能

**浅层神经网络（Shallow Neural Network）**

​	浅层神经网络是一种神经网络，它只有一层或几层隐层，通常用于浅层特征学习和分类任务。与深层神经网络相比，浅层神经网络的参数量少，计算速度较快，但也可能会受到学习能力和表示能力的限制。

**【UL】无监督学习（Unsupervised Learning）**

​	无监督学习是一种机器学习的范畴，其目的是在没有明确的标签或目标函数的情况下，从数据中发现隐藏的结构和规律。在无监督学习中，模型需要从数据中学习如何将数据分组、降维、聚类等。

**【SL】有监督学习（Supervised Learning）**

​	有监督学习是一种机器学习的范畴，其目的是在有标签的训练数据上学习一个模型，使其能够预测新的数据。在有监督学习中，模型需要学习如何将输入映射到输出，并根据标签调整模型参数以提高预测性能。（特定的任务上进行微调，例如情感分析、文本分类、命名实体识别）

**【RL】情景学习（Reinforcement Learning）**

​	情景学习是一种机器学习的范畴，其目的是通过试错来学习如何在特定的情景下做出正确的决策。在情景学习中，模型需要学习如何在不断尝试和反馈的过程中最大化累计奖励。

**【NLP】自然语言处理技术（Natural Language Processing）**

​	自然语言处理技术是一种人工智能技术，旨在让计算机能够理解和生成自然语言。NLP 技术可以应用于文本分类、情感分析、机器翻译、问答系统等领域。

**【LLM】大语言模型（Large Language Model）**

​	大语言模型是一种基于神经网络的自然语言处理模型，它可以处理大量的文本数据，并学习语言的语法和语义。目前的大语言模型通常使用预训练加微调的方式进行训练，并在各种自然语言处理任务中取得了很好的表现。

**【MLLM】多模态大语言模型（Multi modal Large Language Model）**

​	多模态大语言模型是一种结合了自然语言处理和计算机视觉等多种模态的模型，旨在解决多模态数据的处理和理解问题。与传统的大语言模型不同，多模态大语言模型可以同时处理文字、图片、声音等不同类型的数据，并学习它们之间的交互和关联，从而提高自然语言处理和视觉任务的性能。

**RLHF（Reinforcement Learning from Human Feedback 从人类反馈中强化学习）**

​	这是一种机器学习的方法，旨在通过与人类交互来提高强化学习算法的性能。在 RLHF 中，算法会在执行任务的过程中接收人类反馈，并将其作为一种奖励信号来调整自己的策略。与传统的强化学习不同，RLHF 可以在学习过程中快速地获得关于任务的准确信息，并且更容易被应用于实际场景中，如自动驾驶、机器人控制等。



